{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary of Findings\n",
    "\n",
    "\n",
    "## Introduction\n",
    "---\n",
    "The prolem that I would like to look into is the matter of which players will retire before the next season.  This is clearly a classification problem, as I would like my model to classify players into two categories: those who will retire after the given season and those who will not.  In order to measure the model's performance, I will be evaluating its classifications based on f1 score.  I chose to look at this problem from the perspective of a sports jersey merchant.  One would want to be able to prepare to make jerseys for players who will continue to play in the league.  In this sense, incorrectly identifying a player as retiring means you would miss out on potential profits from jersey sales.  Similarly, making a jersey for someone who actually retires (in opposition to your prediction), is a waste of money.  Therefore maximizing one's f1 score is more important than improving overall accuracy. \n",
    "\n",
    "\n",
    "### Results: Baseline Model\n",
    "---\n",
    "For my baseline model, I trained my model on the following variables for the following reasons:\n",
    "\n",
    "    Pos : position (nominal) ; some positions may have different career lengths in the NBA\n",
    "    \n",
    "    Age : age (quantitative) ; as one grows older, they have a greater chance to retire\n",
    "    \n",
    "    Tm : team (nominal) ; some teams may be less willing to extend player contracts\n",
    "    \n",
    "    G : games played (quantitative) ; if someone plays more games, they are more valuable to a team\n",
    "    \n",
    "    GS : games started (quantitative) ; the more games started, the more valuable a player is to a team\n",
    "    \n",
    "    MP : minutes played (quantitative) ; players with more minutes played are less likely to retire\n",
    "    \n",
    "    PTS : points scored (quantitative) ; if a player is scoring more, they are less likely to stop playing\n",
    "    \n",
    "    AST : assists (quantitative) ; if a player has more assists they are more important to a team\n",
    "    \n",
    "    STL : steals (quantitative) ; the more steals a player has, the more defensively valuable they are as a player\n",
    "    \n",
    "    TOV : turnovers (quantitative) ; the more one turns the ball over, the more detrimental they are to the team\n",
    "    \n",
    "    ORB : offensive rebounds (quantitative) ; the more rebounds one has, the more valuable they are to the team\n",
    "    \n",
    "    DRB : defennsive rebounds (quantitative) ; the more rebounds one has, the more valuable they are to the team\n",
    "\n",
    "\n",
    "My base model employed a linear classifier, and performed below what I would have hoped.  Although I had an accuracy of about 88%, my model had f1 scores far lower than what I would have hoped.  On its training set, the model achieved a score of 0.358 and on a test set the model achieved a score of 0.531.  This is poor performance because the scores clue me into the fact that I have poor precision and recall.  Therefore, if I were to be making jerseys I would have a good ammount of jerseys made for players retiring and no jerseys made for some players continuing on.\n",
    "\n",
    "### Results: Final Model\n",
    "---\n",
    "Part of the reason why I believe my first model did not perform well is because I supplied it with too many variales.  Part of improving my model and adding statistics was therefore centered around simplifying parts of my model.\n",
    "\n",
    "Added variables:\n",
    "\n",
    "    MPG : minutes per game ; playtime per game is a better predictor of how used a player is than simply games played or minutes played separate from eachother\n",
    "    \n",
    "    OFF : offensive stats ; this encompases points scored and assists. This metric serves to simplify the information\n",
    "    \n",
    "    DEF : defensive stats ; this encompases stats for regaining posession of the ball\n",
    "    \n",
    "    \n",
    "Variables included in final model: 'MpG', 'DEF', 'OFF', 'GS', 'G', 'Age', 'Tm', 'Pos'\n",
    "\n",
    "\n",
    "After changing which variables my model would be working with, I turned my attention towards which type of classifier would work best.  To do so, I fit the model with a Linear, DecisionTree and KNeighbors classifiers.  Within the decision tree and k-neighbors classifiers I also varied the depth of the tree and the number of neighbors respectively.  After testing these options, I selected to use a DecisionTreeClassifier with a depth of 1, as it gave me the highest f1 score.\n",
    "\n",
    "\n",
    "With these changes, I was able to improve my f1 scores to be 0.599 and 0.616 on my training and test sets respectively.\n",
    "\n",
    "### Results: Fairness Evaluation\n",
    "---\n",
    "I was interested to see if my model became biased towards believing players from a certain team were more likely to retire than others.  This would be a demographic parity.  To test this, I did a permutation test with a test statistic of the ks score between the distribution of teams for players classified as retiring and not retiring.  If our model was biased towards a specific team, the observed ks score would be higher than those from the randomized assignments of retiring vs not retiring.  This was not seen to be the case, and with a p score of 0.962 I can confidently say that my model is not biased. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Code Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plot\n",
    "import seaborn as sns\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading in the tables\n",
    "df = pd.DataFrame(pd.read_html(\"https://www.basketball-reference.com/leagues/NBA_2012_totals.html\")\\\n",
    "                  [0]).drop(['Rk'], axis=1)\n",
    "df.columns = [df.columns[0]] + [str(x) + '_2012_rs' for x in df.columns[1:]]\n",
    "\n",
    "po = pd.DataFrame(pd.read_html(\"https://www.basketball-reference.com/playoffs/NBA_2012_totals.html\")\\\n",
    "                  [0]).drop(['Rk'], axis=1)\n",
    "po.columns = [po.columns[0]] + [str(x) + '_2012_po' for x in po.columns[1:]]\n",
    "\n",
    "\n",
    "df = pd.concat([df,po], sort=True)\n",
    "for i in range(2013, 2019):\n",
    "    rs = pd.DataFrame(pd.read_html(\"https://www.basketball-reference.com/leagues/NBA_\" + str(i) +\\\n",
    "                                   \"_totals.html\")[0]).drop(['Rk'], axis = 1)\n",
    "    rs.columns = [rs.columns[0]] + [str(x) + '_' + str(i) + '_rs' for x in rs.columns[1:]]\n",
    "    df = pd.concat([df,rs], sort=True)\n",
    "    \n",
    "    po = pd.DataFrame(pd.read_html(\"https://www.basketball-reference.com/playoffs/NBA_\" + str(i) + \\\n",
    "                                   \"_totals.html\")[0]).drop(['Rk'], axis = 1)\n",
    "    po.columns = [po.columns[0]] + [str(x) + '_' + str(i) + '_po' for x in po.columns[1:]]\n",
    "    df = pd.concat([df,po], sort=True)\n",
    "\n",
    "#dropping unnecessary rows and converting columns to correct types\n",
    "df = df[df['Player'] != 'Player']\n",
    "df = df.set_index('Player')\n",
    "for col in df.columns:\n",
    "    string = ('Pos' in col) | ('Tm' in col)\n",
    "    if (not string):\n",
    "        df[col] = df[col].astype(float)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a dataframe for players just playing in the given year during the specific season\n",
    "def spec_time(df, year, season='both'):\n",
    "    df = df.filter(like=str(year))\n",
    "    if (season == 'both'):\n",
    "        df = df[~df['Tm_' + str(year) + '_rs'].isnull()]\n",
    "        return df\n",
    "    \n",
    "    df = df.filter(like = str(season))\n",
    "    \n",
    "    len_suff = len('_20XX_xx')\n",
    "    df.columns = [x[:-len_suff] for x in df.columns]\n",
    "    \n",
    "    #only looking at players who were on a team (i.e. playing) that year\n",
    "    df = df[~df['Tm'].isnull()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a list of players who retired after the previous year's season\n",
    "def retired_from_last_season(year):\n",
    "    last = year - 1\n",
    "    last_rs = spec_time(df, last, 'rs')\n",
    "    this_rs = spec_time(df, year, 'rs')\n",
    "    retired = []\n",
    "    \n",
    "    for per in list(last_rs.index):\n",
    "        if (per not in list(this_rs.index)):\n",
    "            if (per not in retired):\n",
    "                retired.append(per)\n",
    "            \n",
    "    return retired\n",
    "\n",
    "#adds a column to a years dataframe of the players who retire after that year\n",
    "def retires_after_this_year(df, year):\n",
    "    ret = retired_from_last_season(year + 1)\n",
    "    retired_y_n = [(x in ret) for x in df.index]\n",
    "    \n",
    "    df['retires'] = retired_y_n\n",
    "    \n",
    "    return df\n",
    "\n",
    "#with players who moved teams mid-season, only consider their combined stat\n",
    "#also replace \"TOT\" team label with 'MOV' to signify transfer\n",
    "def keep_only_total(frame):\n",
    "    df = frame.copy()\n",
    "    df['Player'] = df.index\n",
    "    df = df.drop_duplicates(subset='Player')\n",
    "    df = df.set_index('Player')\n",
    "    \n",
    "    df['Tm'] = df['Tm'].replace('TOT', 'MOV')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_data(year):\n",
    "    #I will train my model on the players which retired after the 2016 season\n",
    "    ret = retired_from_last_season(year + 1)\n",
    "    season = spec_time(df, year, 'rs')\n",
    "\n",
    "    #finding who retires and removing partial stats for season\n",
    "    season = retires_after_this_year(season, year)\n",
    "    season = keep_only_total(season)\n",
    "\n",
    "    #only keeping columns which I think are necessary\n",
    "    keep = ['Pos', 'Age', 'Tm', 'G', 'GS', 'MP', 'PTS', 'AST', 'STL', 'TOV', 'ORB', 'DRB', 'retires']\n",
    "    cols_before = season.columns\n",
    "    for col in cols_before:\n",
    "        if (col not in keep):\n",
    "            season = season.drop(col, axis=1)\n",
    "\n",
    "    return season\n",
    "\n",
    "def calc_f1(prec, rec):\n",
    "    return (2 * prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Baseline Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_16 = get_cleaned_data(2016)\n",
    "#One-hot encoding player position and team\n",
    "oh_cols = ['Tm', 'Pos']\n",
    "#getting all possible team names and positions to propperly one-hot encode\n",
    "all_teams = ['MOV']\n",
    "all_pos = []\n",
    "for i in range(2012, 2018):\n",
    "    yr = spec_time(df, i, 'rs')\n",
    "    \n",
    "    for tm in yr['Tm'].unique():\n",
    "        if (tm not in all_teams):\n",
    "            all_teams.append(tm)\n",
    "            \n",
    "    for po in yr['Pos'].unique():\n",
    "        if (po not in all_pos):\n",
    "            all_pos.append(po)\n",
    "\n",
    "col_trans = ColumnTransformer(transformers=[('one-hot', pp.OneHotEncoder(categories=[all_teams, all_pos]), oh_cols)], \\\n",
    "                              remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nateroberts/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('preprocessing', ColumnTransformer(n_jobs=None, remainder='passthrough', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('one-hot', OneHotEncoder(categorical_features=None,\n",
       "       categories=[['MOV', 'HOU', 'DEN', 'UTA', 'TOR', 'OKC', 'POR', 'PHI', 'BOS', 'ME...m_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl = Pipeline(steps=[('preprocessing', col_trans), ('classify', SGDClassifier())])\n",
    "X = season_16.drop('retires', axis=1)\n",
    "y = season_16.retires\n",
    "\n",
    "pl.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3577235772357723"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f1 for training set\n",
    "preds = pl.predict(X)\n",
    "rec = metrics.recall_score(y, preds)\n",
    "prec = metrics.precision_score(y, preds)\n",
    "calc_f1(prec, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53125"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#f1 tested on 2014 test set\n",
    "\n",
    "season_14 = get_cleaned_data(2014)\n",
    "        \n",
    "X_test = season_14.drop('retires', axis=1)\n",
    "y_test = season_14.retires\n",
    "\n",
    "preds_test = pl.predict(X_test)\n",
    "\n",
    "rec_test = metrics.recall_score(y_test, preds_test)\n",
    "prec_test = metrics.precision_score(y_test, preds_test)\n",
    "calc_f1(prec_test, rec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752598752598753"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Final Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feats(f):\n",
    "    #adding minutes per game stat\n",
    "    frame = f.copy()\n",
    "    \n",
    "    # divided by games plus one in case someone doesnt play a single game\n",
    "    frame['MpG'] = frame['MP'] / (frame['G'] + 1)\n",
    "    \n",
    "    #combining defensive stats together to simplify features\n",
    "    frame['DEF'] = frame['ORB'] + frame['DRB'] + frame['STL']\n",
    "\n",
    "    #combining offensive stats together to simplify features\n",
    "    frame['OFF'] = frame['PTS'] + frame['AST']\n",
    "    \n",
    "    frame = frame[['MpG', 'DEF', 'OFF', 'GS', 'G', 'Age', 'retires', 'Tm', 'Pos']]\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_16 = get_cleaned_data(2016)\n",
    "season_16 = add_feats(season_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nateroberts/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT_1</th>\n",
       "      <td>0.598985</td>\n",
       "      <td>0.460674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KN_20</th>\n",
       "      <td>0.534161</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KN_24</th>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.439024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KN_22</th>\n",
       "      <td>0.509554</td>\n",
       "      <td>0.431694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KN_18</th>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.429708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       train_f1   test_f1\n",
       "DT_1   0.598985  0.460674\n",
       "KN_20  0.534161  0.450000\n",
       "KN_24  0.506494  0.439024\n",
       "KN_22  0.509554  0.431694\n",
       "KN_18  0.537500  0.429708"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing different choices of classifiers\n",
    "X = season_16.drop('retires', axis=1)\n",
    "y = season_16.retires\n",
    "\n",
    "X_test = season_14.drop('retires', axis=1)\n",
    "y_test = season_14.retires\n",
    "\n",
    "train_f1 = []\n",
    "test_f1 = []\n",
    "index_labels = []\n",
    "\n",
    "#Linear classifier\n",
    "pl = Pipeline(steps=[('prep', col_trans), ('classify', SGDClassifier())])\n",
    "pl.fit(X,y)\n",
    "preds = pl.predict(X)\n",
    "rec = metrics.recall_score(y, preds)\n",
    "prec = metrics.precision_score(y, preds)\n",
    "train_f1.append(calc_f1(prec, rec))\n",
    "\n",
    "preds_test = pl.predict(X_test)\n",
    "rec_test = metrics.recall_score(y_test, preds_test)\n",
    "prec_test = metrics.precision_score(y_test, preds_test)\n",
    "test_f1.append(calc_f1(prec_test, rec_test))\n",
    "\n",
    "index_labels.append('Lin')\n",
    "\n",
    "#decision tree with diffferent depths up to 15\n",
    "for i in range(1, 16):\n",
    "    pl = Pipeline(steps=[('prep', col_trans), ('classify', DecisionTreeClassifier(max_depth=i))])\n",
    "    \n",
    "    pl.fit(X,y)\n",
    "    preds = pl.predict(X)\n",
    "    rec = metrics.recall_score(y, preds)\n",
    "    prec = metrics.precision_score(y, preds)\n",
    "    train_f1.append(calc_f1(prec, rec))\n",
    "    \n",
    "    preds_test = pl.predict(X_test)\n",
    "    rec_test = metrics.recall_score(y_test, preds_test)\n",
    "    prec_test = metrics.precision_score(y_test, preds_test)\n",
    "    test_f1.append(calc_f1(prec_test, rec_test))\n",
    "    \n",
    "    index_labels.append('DT_' + str(i))\n",
    "    \n",
    "#KNeighbors with different values of k up to 15\n",
    "for i in range(1, 25):\n",
    "    pl = Pipeline(steps=[('prep', col_trans), ('classify', KNeighborsClassifier(n_neighbors=i))])\n",
    "\n",
    "    pl.fit(X,y)\n",
    "    preds = pl.predict(X)\n",
    "    rec = metrics.recall_score(y, preds)\n",
    "    prec = metrics.precision_score(y, preds)\n",
    "    train_f1.append(calc_f1(prec, rec))\n",
    "    \n",
    "    preds_test = pl.predict(X_test)\n",
    "    rec_test = metrics.recall_score(y_test, preds_test)\n",
    "    prec_test = metrics.precision_score(y_test, preds_test)\n",
    "    test_f1.append(calc_f1(prec_test, rec_test))\n",
    "    \n",
    "    index_labels.append('KN_' + str(i))\n",
    "    \n",
    "results = pd.DataFrame()\n",
    "results['train_f1'] = train_f1\n",
    "results['test_f1'] = test_f1\n",
    "results.index = index_labels\n",
    "results.sort_values('test_f1', ascending = False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5989847715736041"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = season_16.drop('retires', axis=1)\n",
    "y = season_16.retires\n",
    "pl = Pipeline(steps=[('prep', col_trans), ('classify', DecisionTreeClassifier(max_depth=1))])\n",
    "pl.fit(X,y)\n",
    "\n",
    "X_test = add_feats(season_14).drop('retires', axis=1)\n",
    "y_test = season_14.retires\n",
    "\n",
    "#the accuracy of my model on its training set\n",
    "preds = pl.predict(X)\n",
    "rec = metrics.recall_score(y, preds)\n",
    "prec = metrics.precision_score(y, preds)\n",
    "calc_f1(prec, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6161137440758294"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = pl.predict(X_test)\n",
    "rec = metrics.recall_score(y_test, preds_test)\n",
    "prec = metrics.precision_score(y_test, preds_test)\n",
    "calc_f1(prec, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Fairness Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is my model biased towards believing players from a certain team are more likely to stop playing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05521844660194175"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = pl.predict(X).astype(int)\n",
    "season_16['predicted'] = preds\n",
    "grps = season_16.groupby('predicted')['Tm']\n",
    "ind = season_16.index\n",
    "obs = ks_2samp(grps.get_group(1), grps.get_group(0)).statistic\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_repetitions = 500\n",
    "\n",
    "ks_list = []\n",
    "for _ in range(n_repetitions):\n",
    "    \n",
    "    # shuffle the gender column\n",
    "    shuffled_col = (\n",
    "        season_16['predicted']\n",
    "        .sample(replace=False, frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # put them in a table\n",
    "    shuffled_col = pd.Series(shuffled_col)\n",
    "    shuffled_col.index = ind\n",
    "    season_16['predicted'] = shuffled_col\n",
    "    shuffled = (\n",
    "        season_16\n",
    "        .assign(**{\n",
    "            'predicted': shuffled_col,\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    # compute the KS\n",
    "    grps = shuffled.groupby('predicted')['Tm']\n",
    "    ks = ks_2samp(grps.get_group(1), grps.get_group(0)).statistic\n",
    "    \n",
    "    ks_list.append(ks)\n",
    "    \n",
    "\n",
    "pval = np.mean(ks_list > obs)\n",
    "\n",
    "pd.Series(ks_list).plot(kind='hist', density=True, alpha=0.8, title='p-value: %f' % pval)\n",
    "\n",
    "plot.scatter(obs, 0, color='red', s=40);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
